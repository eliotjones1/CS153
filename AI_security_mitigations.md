# Mitigations

As discussed in the previous section, GenAI is potentially a very powerful tool for democratizing access to hacking. Its generative capabilities allow it to provide technical support to bad actors with malicious intents who might not have the requisite hacking capabilities. Further, while LLMs are proven to be excellent when it comes to analyzing and writing code, other models that are trained to produce audio and visual content are already proving to be the next generation of offensive social engineering and phishing tools. Deepfaked images, video, and spoofed audio are becoming increasingly convincing, even to the well-trained eye. As with any toolset, the creators must be conscious of all use cases – for good and bad – when choosing to publish said tool to the public. As a result, the majority of the responsibility for mitigating these risks falls on the shoulders of the organizations that create these tools, as well as national or international governing bodies for regulating the GenAI industry. 

## Organizational Level

At an organizational level, approaches to securing LLMs or other GenAI models come in three phases: the pre-model phase, the model phase, and the post-model phase. The pre-model phase denotes the period before the training of the model, when datasets are being curated, created, and cleaned prior to being used for training. The model phase represents the combination of the pretraining and fine tuning phases, and the post-model phase considers safeguards on how the outside world interacts with the model, such as through prompt filtering. There is also a fourth category which has made its way into the conversation recently, after Anthropic’s findings on the interpretability of some of their model’s features – this will be called the “model understanding” phase.

### The Pre-Model Phase

Different organizations take different approaches to how they mitigate the potential harmful impacts of their GenAI models, and these differences are perhaps the most clear during this phase. For example, prior to training their DALL-E model, OpenAI removed “the most explicit content from the training data,”\cite{dalle2} thereby, according to them, minimizing the model’s exposure to, and knowledge about, those topics. However, unlike OpenAI, it is unclear if many of the other most popular text-to-image generation models utilize the same sort of filtering. For example, Stable Diffusion utilizes \cite{mcgill2022exploring} subsets of the LAION-5B dataset, a dataset of images scraped from the web. However, as the curators of LAION stated in their release \cite{laionLAION5BOPEN}, “Be aware that this large-scale dataset is uncurated. Keep in mind that the uncurated nature of the dataset means that collected links may lead to strongly discomforting and disturbing content for a human viewer.” Even worse, a study \cite{404mediaLargestDataset} by Stanford’s Internet Observatory found that the dataset contained samples of child sexual abuse material, prompting LAION to take down the datasets. 

Compared to the current state of the art LLMs, these text-to-image generation models are relatively small. Llama-3, for example \cite{metaIntroducingMeta}, was pretrained on over 15 trillion tokens, compared to the around 2.3 billion that Stable Diffusion was initially trained on. This means that mitigations in this phase are much more feasible for smaller models that are used to generate images and video than they are for large language models. In this phase, OpenAI’s method of rigorously filtering the dataset before training represents the industry best practice for smaller models. 

### The Model Phase
LLMs and other GenAI models often go through an initial stage of pretraining, followed by a fine tuning round, before being put into production. It is during this phase, especially in the fine tuning mode, where the most significant safety strides are made by most organizations. For example, in their technical paper \cite{openai2024gpt4} on the release of GPT-4, the authors describe their RLHF (reinforcement learning with human feedback) pipeline, in which humans are inserted into the training pipeline in order to guide the model towards more desirable outputs. They also utilize a system of GPT-4 classifiers which are used to reward the model for refusing to generate unsafe outputs. More recently, in their release \cite{metaIntroducingMeta} of Llama-3, Meta describes a similar process, whereby they utilize a “red teaming approach” consisting of both human experts, as well as some automated methods, to try to generate prompts that will elicit an undesirable or unsafe response from the model. 

However, finetuning, when left to the hands of the users, can prove to circumvent the safeguards put in place by these organizations. This \cite{qi2023finetuning} paper by a group of researchers from Stanford, Princeton, Virginia Tech, and IBM, proved that models’ built-in safeguards can be circumnavigated by as little as 10 adversarial training examples. Yet another conclusion made in the same paper was that even fine-tuning with alternative, commonly-used datasets, can reduce the efficacy of these safeguards. 

### The Post-Model Phase 
The post-model phase can be further subdivided into two parts. Firstly, once a model has been trained and fine tuned, organizations typically run a suite of tests to determine the risks present. The best such example currently is Meta’s CyberSecEval2 \cite{bhatt2024cyberseceval}, which is their own benchmark used to determine the risks posed by their in-house models. This benchmark covers five categories: “insecure coding,” “cyberattack helpfulness,” “prompt injection,” “vulnerability identification and exploitation,” and “code interpreter abuse.” These categories each include a series of prompts meant to test the model’s resilience to prompts posed by actors with malicious intents. As mentioned in the introduction, another common form of benchmarking LLMs specifically is by using CTFs to evaluate the model’s ability to solve hacking challenges. However, at present the current state-of-the-art in this category is a collection of PicoCTF problems, which have been mentioned previously as purely educational in nature, aimed at middle and high school students, thus highlighting the lack of a true benchmark in this case. 

Further, any GenAI platform will come with built-in safeguards on the prompting side. DALL-E, for example, will refuse to create images of public figures in a negative light, just like GPT-4, Llama-3, and all of the other premier LLMs have a prompt filter that prevents the models from even getting access to prompts deemed to be malicious in their intent. These safeguards are incredibly important when it comes to preventing users from gaining hacking knowledge, but malicious knowledge in general, as they are the first line of defense for these models. 
### The Model Understanding Phase 
In light of Anthropic’s breakthrough \cite{techrepublicAnthropicsGenerative} research on LLM security and bias, this fourth category also exists as an important mitigation strategy. The researchers found a way to see which features were present as a model was generating responses, which has massive impacts on LLM safety and security. One such example they provided was the topical feature “backdoors,” in reference to backdoors in systems, which can be found in responses to questions or prompts about hidden cameras. However, these features are largely controllable with what they called “clamping,” or a method of retroactively manually tuning the weights of these features after training. Tuning down some of these threatening parameters resulted in less biased or hateful speech. 

## Governmental Regulation 
At the regulatory level, there have been trends towards stricter guidelines for the ethical use of GenAI within the cybersecurity field. The European Union, for example, passed the AI Act \cite{artificialintelligenceactArtificialIntelligence}, which seeks to specifically regulate systems which fall into two of their predefined categories: systems that create an “unacceptable risk,” and systems which have “high-risk applications.” Systems that create “unacceptable risk,” such as social-scoring systems used in China, are banned, while systems with “high-risk applications,” like ones that scan C.V.s during a hiring process, are highly regulated. Further, the National Institute of Standards and Technology (NIST) in the United States is currently working on developing a framework to ensure the safety and reliability of AI systems \cite{nistNationalInstitute}. However, it is unclear whether or not these types of regulations will prove to be strict enough, due to their lack of specificity and non-technical language.

## Best Practices 
While the importance of governmental regulation cannot be understated, the responsibility for mitigating these risks ultimately lies with the organizations who create GenAI models. The best methods for mitigation at the organizational level are: \
    - Removing harmful content from the initial datasets, in order to limit model exposure from the beginning. \
    - Utilizing RLHF during the fine-tuning stage to help guide models towards more user-friendly outputs. \
    - Intensive benchmarking and red-teaming after the first two phases, in order to ensure the safety and security of the model in the real world. \
    - Implementing safeguards into the user interaction component, in order to prevent malicious actors from prompting the model into outputting something against company or governmental policy. \
    - Understanding the specifics of the model’s architecture and response patterns, in order to potentially catch any biases which might cause dangerous responses. 
